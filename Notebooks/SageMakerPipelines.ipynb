{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, learn how to create and automate end-to-end machine learning (ML) workflows using Amazon SageMaker Pipelines, Amazon SageMaker Model Registry, and Amazon SageMaker Clarify.\n",
    "\n",
    "SageMaker Pipelines is the first purpose-built continuous integration and continuous delivery (CI/CD) service for ML. With SageMaker Pipelines, you can automate different steps of the ML workflow, including data loading, data transformation, training, tuning, evaluation, and deployment. SageMaker Model Registry allows you to track model versions, their metadata such as use case grouping, and model performance metrics baselines in a central repository where it is easy to choose the right model for deployment based on your business requirements. SageMaker Clarify provides greater visibility into your training data and models so you can identify and limit bias and explain predictions.\n",
    "\n",
    "In this tutorial, you will implement a SageMaker pipeline to build, train, and deploy an XGBoost binary classification model that predicts the likelihood of an auto insurance claim being fraudulent. You will use a synthetically generated auto insurance claims dataset. The raw inputs are two tables of insurance data: a claims table and a customers table. The claims table has a column named fraud indicating whether a claim was fraudulent or otherwise. Your pipeline will process the raw data; create training, validation, and test datasets; and build and evaluate a binary classification model. It will then use SageMaker Clarify to test model bias and explainability, and lastly deploy the model for inference.\n",
    "\n",
    "## What you will accomplish\n",
    "In this guide, you will:\n",
    "\n",
    "- Build and run a SageMaker pipeline to automate the end-to-end ML lifecyle\n",
    "- Generate predictions using the deployed model\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this guide, you will need:\n",
    "- An AWS account: If you don't already have an account, follow the Setting Up Your AWS Environment getting started guide for a quick overview.\n",
    "\n",
    "## Implementation\n",
    "We are going to use **Data Science Python 3 Kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput, \n",
    "    ScriptProcessor\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, \n",
    "    TrainingStep, \n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger, \n",
    "    ParameterFloat, \n",
    "    ParameterString, \n",
    "    ParameterBoolean\n",
    ")\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    ModelBiasCheckConfig, \n",
    "    ClarifyCheckStep, \n",
    "    ModelExplainabilityCheckConfig\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource, \n",
    "    ModelMetrics, \n",
    "    FileSource\n",
    ")\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "\n",
    "from sagemaker.image_uris import retrieve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, We are going to define the location of the data inside S3 Buckets, including the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate AWS services session and client objects\n",
    "sess = sagemaker.Session()\n",
    "write_bucket = sess.default_bucket()\n",
    "write_prefix = \"fraud-detect-demo\"\n",
    "\n",
    "region = sess.boto_region_name\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Fetch SageMaker execution role\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# S3 locations used for parameterizing the notebook run\n",
    "read_bucket = \"sagemaker-sample-files\"\n",
    "read_prefix = \"datasets/tabular/synthetic_automobile_claims\" \n",
    "\n",
    "# S3 location where raw data to be fetched from\n",
    "raw_data_key = f\"s3://{read_bucket}/{read_prefix}\"\n",
    "\n",
    "# S3 location where processed data to be uploaded\n",
    "processed_data_key = f\"{write_prefix}/processed\"\n",
    "\n",
    "# S3 location where train data to be uploaded\n",
    "train_data_key = f\"{write_prefix}/train\"\n",
    "\n",
    "# S3 location where validation data to be uploaded\n",
    "validation_data_key = f\"{write_prefix}/validation\"\n",
    "\n",
    "# S3 location where test data to be uploaded\n",
    "test_data_key = f\"{write_prefix}/test\"\n",
    "\n",
    "\n",
    "# Full S3 paths\n",
    "claims_data_uri = f\"{raw_data_key}/claims.csv\"\n",
    "customers_data_uri = f\"{raw_data_key}/customers.csv\"\n",
    "output_data_uri = f\"s3://{write_bucket}/{write_prefix}/\"\n",
    "scripts_uri = f\"s3://{write_bucket}/{write_prefix}/scripts\"\n",
    "estimator_output_uri = f\"s3://{write_bucket}/{write_prefix}/training_jobs\"\n",
    "processing_output_uri = f\"s3://{write_bucket}/{write_prefix}/processing_jobs\"\n",
    "model_eval_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_eval\"\n",
    "clarify_bias_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/bias_config\"\n",
    "clarify_explainability_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/explainability_config\"\n",
    "bias_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/bias\"\n",
    "explainability_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/explainability\"\n",
    "\n",
    "# Retrieve training image\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.3-1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following code is to set the names for the various **SageMaker pipeline components**, such as the model and the endpoint, and specify training and inference instance types and counts. These values will be used to parametrize your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names of pipeline objects\n",
    "pipeline_name = \"FraudDetectXGBPipeline\"\n",
    "pipeline_model_name = \"fraud-detect-xgb-pipeline\"\n",
    "model_package_group_name = \"fraud-detect-xgb-model-group\"\n",
    "base_job_name_prefix = \"fraud-detect\"\n",
    "endpoint_config_name = f\"{pipeline_model_name}-endpoint-config\"\n",
    "endpoint_name = f\"{pipeline_model_name}-endpoint\"\n",
    "\n",
    "# Set data parameters\n",
    "target_col = \"fraud\"\n",
    "\n",
    "# Set instance types and counts\n",
    "process_instance_type = \"ml.c5.xlarge\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.m4.xlarge\"\n",
    "clarify_instance_count = 1\n",
    "clarify_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Pipelines supports **parameterization**, which allows you to specify input parameters at runtime without changing your pipeline code. You can use the modules available under the **sagemaker.workflow.parameters** module, such as **ParameterInteger, ParameterFloat, ParameterString**, and **ParameterBoolean**, to specify pipeline parameters of various data types. Copy, paste, and run the following code to set up multiple input parameters, including SageMaker Clarify configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline input parameters\n",
    "\n",
    "# Set processing instance type\n",
    "process_instance_type_param = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=process_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance type\n",
    "train_instance_type_param = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=train_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance count\n",
    "train_instance_count_param = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=train_instance_count\n",
    ")\n",
    "\n",
    "# Set deployment instance type\n",
    "deploy_instance_type_param = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=predictor_instance_type,\n",
    ")\n",
    "\n",
    "# Set deployment instance count\n",
    "deploy_instance_count_param = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\",\n",
    "    default_value=predictor_instance_count\n",
    ")\n",
    "\n",
    "# Set Clarify check instance type\n",
    "clarify_instance_type_param = ParameterString(\n",
    "    name=\"ClarifyInstanceType\",\n",
    "    default_value=clarify_instance_type,\n",
    ")\n",
    "\n",
    "# Set model bias check params\n",
    "skip_check_model_bias_param = ParameterBoolean(\n",
    "    name=\"SkipModelBiasCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_bias_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelBiasBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_bias_param = ParameterString(\n",
    "    name=\"ModelBiasSuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model explainability check params\n",
    "skip_check_model_explainability_param = ParameterBoolean(\n",
    "    name=\"SkipModelExplainabilityCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_explainability_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelExplainabilityBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_explainability_param = ParameterString(\n",
    "    name=\"ModelExplainabilitySuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model approval param\n",
    "model_approval_status_param = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"Approved\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the pipeline components\n",
    "\n",
    "A pipeline is a sequence of steps that can be individually built and then put together to form an ML workflow. The following diagram shows the high-level steps of a pipeline.\n",
    "\n",
    "![MLOps Flow](/Images/SMPipeline/Notebook/1.png)\n",
    "\n",
    "In this tutorial, you build a pipeline with the following steps:\n",
    "\n",
    "1. Data processing step: Runs a SageMaker Processing job using the input raw data in S3 and outputs training, validation, and test splits to S3.\n",
    "2. Training step: Trains an XGBoost model using SageMaker training jobs with training and validation data in S3 as inputs, and stores the trained model artifact in S3.\n",
    "3. Evaluation step: Evaluates the model on the test dataset by running a SageMaker Processing job using the test data and the model artifact in S3 as inputs, and stores the output model performance evaluation report in S3.\n",
    "4. Conditional step: Compares model performance on the test dataset against the threshold. Runs a SageMaker Pipelines predefined step using the model performance evaluation report in S3 as input, and stores the output list of pipeline steps that will be executed if model performance is acceptable.\n",
    "5. Create model step: Runs a SageMaker Pipelines predefined step using the model artifact in S3 as an input, and stores the output SageMaker model in S3.\n",
    "6. Bias check step: Checks for model bias using SageMaker Clarify with the training data and model artifact in S3 as inputs and stores the model bias report and baseline metrics in S3.\n",
    "7. Model explainability step: Runs SageMaker Clarify with the training data and model artifact in S3 as inputs, and stores the model explainability report and baseline metrics in S3.\n",
    "8. Register step: Runs a SageMaker Pipelines predefined step using the model, bias, and explainability baseline metrics as inputs to register the model in the SageMaker Model Registry.\n",
    "9. Deploy step: Runs a SageMaker Pipelines predefined step using an AWS Lambda handler function, the model, and the endpoint configuration as inputs to deploy the model to a SageMaker Real-Time Inference endpoint.\n",
    "\n",
    "SageMaker Pipelines provides many predefined step types, such as steps for data processing, model training, model tuning, and batch transformation. For more information, see Pipeline Steps in the Amazon SageMaker Developer Guide. In the following steps, you configure and define each pipeline step individually, and then define the pipeline itself by combining the pipeline steps with the input parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Step\n",
    "In this step you prepare a Python script to ingest raw files; perform processing such as missing values imputation and feature engineering; and curate the training, validation, and test splits to be used for model building. Copy, paste, and run the following code to build your processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import pathlib\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-ratio\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--validation-ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--test-ratio\", type=float, default=0.1)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    logger.info(\"Received arguments {}\".format(args))\n",
    "    \n",
    "    # Set local path prefix in the processing container\n",
    "    local_dir = \"/opt/ml/processing\"    \n",
    "    \n",
    "    input_data_path_claims = os.path.join(\"/opt/ml/processing/claims\", \"claims.csv\")\n",
    "    input_data_path_customers = os.path.join(\"/opt/ml/processing/customers\", \"customers.csv\")\n",
    "    \n",
    "    logger.info(\"Reading claims data from {}\".format(input_data_path_claims))\n",
    "    df_claims = pd.read_csv(input_data_path_claims)\n",
    "    \n",
    "    logger.info(\"Reading customers data from {}\".format(input_data_path_customers))\n",
    "    df_customers = pd.read_csv(input_data_path_customers)\n",
    "    \n",
    "    logger.debug(\"Formatting column names.\")\n",
    "    # Format column names\n",
    "    df_claims = df_claims.rename({c : c.lower().strip().replace(' ', '_') for c in df_claims.columns}, axis = 1)\n",
    "    df_customers = df_customers.rename({c : c.lower().strip().replace(' ', '_') for c in df_customers.columns}, axis = 1)\n",
    "    \n",
    "    logger.debug(\"Joining datasets.\")\n",
    "    # Join datasets\n",
    "    df_data = df_claims.merge(df_customers, on = 'policy_id', how = 'left')\n",
    "\n",
    "    # Drop selected columns not required for model building\n",
    "    df_data = df_data.drop(['customer_zip'], axis = 1)\n",
    "    \n",
    "    # Select Ordinal columns\n",
    "    ordinal_cols = [\"police_report_available\", \"policy_liability\", \"customer_education\"]\n",
    "\n",
    "    # Select categorical columns and filling with na\n",
    "    cat_cols_all = list(df_data.select_dtypes('object').columns)\n",
    "    cat_cols = [c for c in cat_cols_all if c not in ordinal_cols]\n",
    "    df_data[cat_cols] = df_data[cat_cols].fillna('na')\n",
    "    \n",
    "    logger.debug(\"One-hot encoding categorical columns.\")\n",
    "    # One-hot encoding categorical columns\n",
    "    df_data = pd.get_dummies(df_data, columns = cat_cols)\n",
    "    \n",
    "    logger.debug(\"Encoding ordinal columns.\")\n",
    "    # Ordinal encoding\n",
    "    mapping = {\n",
    "               \"Yes\": \"1\",\n",
    "               \"No\": \"0\" \n",
    "              }\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].map(mapping)\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"15/30\": \"0\",\n",
    "               \"25/50\": \"1\", \n",
    "               \"30/60\": \"2\",\n",
    "               \"100/200\": \"3\"\n",
    "              }\n",
    "    \n",
    "    df_data['policy_liability'] = df_data['policy_liability'].map(mapping)\n",
    "    df_data['policy_liability'] = df_data['policy_liability'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"Below High School\": \"0\",\n",
    "               \"High School\": \"1\", \n",
    "               \"Associate\": \"2\",\n",
    "               \"Bachelor\": \"3\",\n",
    "               \"Advanced Degree\": \"4\"\n",
    "              }\n",
    "    \n",
    "    df_data['customer_education'] = df_data['customer_education'].map(mapping)\n",
    "    df_data['customer_education'] = df_data['customer_education'].astype(float)\n",
    "    \n",
    "    df_processed = df_data.copy()\n",
    "    df_processed.columns = [c.lower() for c in df_data.columns]\n",
    "    df_processed = df_processed.drop([\"policy_id\", \"customer_gender_unkown\"], axis=1)\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    train_ratio = args.train_ratio\n",
    "    val_ratio = args.validation_ratio\n",
    "    test_ratio = args.test_ratio\n",
    "    \n",
    "    logger.debug(\"Splitting data into train, validation, and test sets\")\n",
    "    \n",
    "    y = df_processed['fraud']\n",
    "    X = df_processed.drop(['fraud'], axis = 1)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    train_df = pd.concat([y_train, X_train], axis = 1)\n",
    "    val_df = pd.concat([y_val, X_val], axis = 1)\n",
    "    test_df = pd.concat([y_test, X_test], axis = 1)\n",
    "    dataset_df = pd.concat([y, X], axis = 1)\n",
    "    \n",
    "    logger.info(\"Train data shape after preprocessing: {}\".format(train_df.shape))\n",
    "    logger.info(\"Validation data shape after preprocessing: {}\".format(val_df.shape))\n",
    "    logger.info(\"Test data shape after preprocessing: {}\".format(test_df.shape))\n",
    "    \n",
    "    # Save processed datasets to the local paths in the processing container.\n",
    "    # SageMaker will upload the contents of these paths to S3 bucket\n",
    "    logger.debug(\"Writing processed datasets to container local path.\")\n",
    "    train_output_path = os.path.join(f\"{local_dir}/train\", \"train.csv\")\n",
    "    validation_output_path = os.path.join(f\"{local_dir}/val\", \"validation.csv\")\n",
    "    test_output_path = os.path.join(f\"{local_dir}/test\", \"test.csv\")\n",
    "    full_processed_output_path = os.path.join(f\"{local_dir}/full\", \"dataset.csv\")\n",
    "\n",
    "    logger.info(\"Saving train data to {}\".format(train_output_path))\n",
    "    train_df.to_csv(train_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving validation data to {}\".format(validation_output_path))\n",
    "    val_df.to_csv(validation_output_path, index=False)\n",
    "\n",
    "    logger.info(\"Saving test data to {}\".format(test_output_path))\n",
    "    test_df.to_csv(test_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving full processed data to {}\".format(full_processed_output_path))\n",
    "    dataset_df.to_csv(full_processed_output_path, index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following code block to **instantiate the processor** and the SageMaker Pipelines step to run the processing script. Since the processing script is written in Pandas, you use a **SKLearnProcessor**. The SageMaker Pipelines **ProcessingStep** function takes the following arguments: the processor, the input S3 locations for raw datasets, and the output S3 locations to save processed datasets. Additional arguments such as training, validation, and test split ratios are provided through the **job_arguments** argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "# Upload processing script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"preprocessing.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/preprocessing.py\"\n",
    ")\n",
    "\n",
    "# Define the SKLearnProcessor configuration\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=process_instance_type,\n",
    "    base_job_name=f\"{base_job_name_prefix}-processing\",\n",
    ")\n",
    "\n",
    "# Define pipeline processing step\n",
    "process_step = ProcessingStep(\n",
    "    name=\"DataProcessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=claims_data_uri, destination=\"/opt/ml/processing/claims\"),\n",
    "        ProcessingInput(source=customers_data_uri, destination=\"/opt/ml/processing/customers\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/train_data\", output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/validation_data\", output_name=\"validation_data\", source=\"/opt/ml/processing/val\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/test_data\", output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/processed_data\", output_name=\"processed_data\", source=\"/opt/ml/processing/full\")\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--train-ratio\", \"0.8\", \n",
    "        \"--validation-ratio\", \"0.1\",\n",
    "        \"--test-ratio\", \"0.1\"\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/preprocessing.py\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step\n",
    "\n",
    "Run the following code block to prepare the training script. This script encapsulates the training logic for the XGBoost binary classifier. Hyperparameters used in model training are provided later in the tutorial through the training step definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile xgboost_train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters and algorithm parameters are described here\n",
    "    parser.add_argument(\"--num_round\", type=int, default=100)\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=3)\n",
    "    parser.add_argument(\"--eta\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--colsample_bytree\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--objective\", type=str, default=\"binary:logistic\")\n",
    "    parser.add_argument(\"--eval_metric\", type=str, default=\"auc\")\n",
    "    parser.add_argument(\"--nfold\", type=int, default=3)\n",
    "    parser.add_argument(\"--early_stopping_rounds\", type=int, default=3)\n",
    "    \n",
    "\n",
    "    # SageMaker specific arguments. Defaults are set in the environment variables\n",
    "    # Set location of input training data\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    # Set location of input validation data\n",
    "    parser.add_argument(\"--validation_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    # Set location where trained model will be stored. Default set by SageMaker, /opt/ml/model\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    # Set location where model artifacts will be stored. Default set by SageMaker, /opt/ml/output/data\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data_train = pd.read_csv(f\"{args.train_data_dir}/train.csv\")\n",
    "    train = data_train.drop(\"fraud\", axis=1)\n",
    "    label_train = pd.DataFrame(data_train[\"fraud\"])\n",
    "    dtrain = xgb.DMatrix(train, label=label_train)\n",
    "    \n",
    "    \n",
    "    data_validation = pd.read_csv(f\"{args.validation_data_dir}/validation.csv\")\n",
    "    validation = data_validation.drop(\"fraud\", axis=1)\n",
    "    label_validation = pd.DataFrame(data_validation[\"fraud\"])\n",
    "    dvalidation = xgb.DMatrix(validation, label=label_validation)\n",
    "    \n",
    "    # Choose XGBoost model hyperparameters\n",
    "    params = {\"max_depth\": args.max_depth,\n",
    "              \"eta\": args.eta,\n",
    "              \"objective\": args.objective,\n",
    "              \"subsample\" : args.subsample,\n",
    "              \"colsample_bytree\":args.colsample_bytree\n",
    "             }\n",
    "    \n",
    "    num_boost_round = args.num_round\n",
    "    nfold = args.nfold\n",
    "    early_stopping_rounds = args.early_stopping_rounds\n",
    "    \n",
    "    # Cross-validate train XGBoost model\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        nfold=nfold,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        metrics=[\"auc\"],\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=len(cv_results))\n",
    "    \n",
    "    train_pred = model.predict(dtrain)\n",
    "    validation_pred = model.predict(dvalidation)\n",
    "    \n",
    "    train_auc = roc_auc_score(label_train, train_pred)\n",
    "    validation_auc = roc_auc_score(label_validation, validation_pred)\n",
    "    \n",
    "    print(f\"[0]#011train-auc:{train_auc:.2f}\")\n",
    "    print(f\"[0]#011validation-auc:{validation_auc:.2f}\")\n",
    "\n",
    "    metrics_data = {\"hyperparameters\" : params,\n",
    "                    \"binary_classification_metrics\": {\"validation:auc\": {\"value\": validation_auc},\n",
    "                                                      \"train:auc\": {\"value\": train_auc}\n",
    "                                                     }\n",
    "                   }\n",
    "              \n",
    "    # Save the evaluation metrics to the location specified by output_data_dir\n",
    "    metrics_location = args.output_data_dir + \"/metrics.json\"\n",
    "    \n",
    "    # Save the trained model to the location specified by model_dir\n",
    "    model_location = args.model_dir + \"/xgboost-model\"\n",
    "\n",
    "    with open(metrics_location, \"w\") as f:\n",
    "        json.dump(metrics_data, f)\n",
    "\n",
    "    with open(model_location, \"wb\") as f:\n",
    "        joblib.dump(model, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model training using a SageMaker **XGBoost estimator** and the SageMaker Pipelines **TrainingStep** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set XGBoost model hyperparameters \n",
    "hyperparams = {  \n",
    "    \"eval_metric\" : \"auc\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"5\",\n",
    "    \"max_depth\":\"5\",\n",
    "    \"subsample\":\"0.75\",\n",
    "    \"colsample_bytree\":\"0.75\",\n",
    "    \"eta\":\"0.5\"\n",
    "}\n",
    "\n",
    "# Set XGBoost estimator\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"xgboost_train.py\", \n",
    "    output_path=estimator_output_uri,\n",
    "    code_location=estimator_output_uri,\n",
    "    hyperparameters=hyperparams,\n",
    "    role=sagemaker_role,\n",
    "    # Fetch instance type and count from pipeline parameters\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type,\n",
    "    framework_version=\"1.3-1\"\n",
    ")\n",
    "\n",
    "# Access the location where the preceding processing step saved train and validation datasets\n",
    "# Pipeline step properties can give access to outputs which can be used in succeeding steps\n",
    "s3_input_train = TrainingInput(\n",
    "    s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri, \n",
    "    content_type=\"csv\", \n",
    "    s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\"validation_data\"].S3Output.S3Uri,\n",
    "    content_type=\"csv\",\n",
    "    s3_data_type=\"S3Prefix\"\n",
    ")\n",
    "\n",
    "# Set pipeline training step\n",
    "train_step = TrainingStep(\n",
    "    name=\"XGBModelTraining\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "    \"train\":s3_input_train, # Train channel \n",
    "    \"validation\": s3_input_validation # Validation channel\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Step\n",
    "\n",
    "run the following code block, which will be used to create a SageMaker model using the SageMaker Pipelines **CreateModelStep** function. This step utilizes the output of the training step to package the model for deployment. Note that the value for the instance type argument is passed using the SageMaker Pipelines parameter you defined earlier in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker model\n",
    "model = sagemaker.model.Model(\n",
    "    image_uri=training_image,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "# Specify model deployment instance type\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=deploy_instance_type_param)\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"FraudDetModel\", model=model, inputs=inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Checking Step\n",
    "\n",
    "In an ML workflow, it is important to assess a trained model for potential biases and understand how the various features in the input data affect model prediction. SageMaker Pipelines provides a **ClarifyCheckStep** function that can be used to perform three types of checks: data bias check (pre-training), model bias check (post-training), and model explainability check. To reduce the running time, in this tutorial, you implement only bias and explainability checks. Copy, paste, and run the following code block to set up SageMaker Clarify for model bias check. Note that this step picks up assets, such as the training data and the SageMaker model created in the previous steps, through the **properties** attribute. When the pipeline is executed, this step isn't started until after the steps providing the inputs finish running. For more details, see [Data Dependency Between Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#build-and-manage-data-dependency) in the Amazon SageMaker Developer Guide. To manage costs and the tutorial running time, the **ModelBiasCheckConfig** function is configured to calculate only one bias metric, **DPPL**. For more information about the bias metrics available in SageMaker Clarify, see [Measure Posttraining Data and Model Bias](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-post-training-bias.html) in the Amazon SageMaker Developer Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up common configuration parameters to be used across multiple steps\n",
    "check_job_config = CheckJobConfig(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=clarify_instance_type,\n",
    "    volume_size_in_gb=30,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Set up configuration of data to be used for model bias check\n",
    "model_bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    # Fetch S3 location where processing step saved train data\n",
    "    s3_data_input_path=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "    s3_output_path=bias_report_output_uri,\n",
    "    label=target_col,\n",
    "    dataset_type=\"text/csv\",\n",
    "    s3_analysis_config_output_path=clarify_bias_config_output_uri\n",
    ")\n",
    "\n",
    "# Set up details of the trained model to be checked for bias\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    # Pull model name from model creation step\n",
    "    model_name=create_model_step.properties.ModelName,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type\n",
    ")\n",
    "\n",
    "# Set up column and categories that are to be checked for bias\n",
    "model_bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[0],\n",
    "    facet_name=\"customer_gender_female\",\n",
    "    facet_values_or_threshold=[1]\n",
    ")\n",
    "\n",
    "# Set up model predictions configuration to get binary labels from probabilities\n",
    "model_predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)\n",
    "\n",
    "model_bias_check_config = ModelBiasCheckConfig(\n",
    "    data_config=model_bias_data_config,\n",
    "    data_bias_config=model_bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=model_predictions_config,\n",
    "    methods=[\"DPPL\"]\n",
    ")\n",
    "\n",
    "# Set up pipeline model bias check step\n",
    "model_bias_check_step = ClarifyCheckStep(\n",
    "    name=\"ModelBiasCheck\",\n",
    "    clarify_check_config=model_bias_check_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_bias_param,\n",
    "    register_new_baseline=register_new_baseline_model_bias_param,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_bias_param\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability Step\n",
    "\n",
    "Run the following code block to set up model explainability checks. This step provides insights such as feature importance (how input features impact model predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration of data to be used for model explainability check\n",
    "model_explainability_data_config = sagemaker.clarify.DataConfig(\n",
    "    # Fetch S3 location where processing step saved train data\n",
    "    s3_data_input_path=process_step.properties.ProcessingOutputConfig.Outputs[\"train_data\"].S3Output.S3Uri,\n",
    "    s3_output_path=explainability_report_output_uri,\n",
    "    label=target_col,\n",
    "    dataset_type=\"text/csv\",\n",
    "    s3_analysis_config_output_path=clarify_explainability_config_output_uri \n",
    ")\n",
    "\n",
    "# Set SHAP configuration for Clarify to compute global and local SHAP values for feature importance\n",
    "shap_config = sagemaker.clarify.SHAPConfig(\n",
    "    seed=42, \n",
    "    num_samples=100,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=True\n",
    ")\n",
    "\n",
    "model_explainability_config = ModelExplainabilityCheckConfig(\n",
    "    data_config=model_explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config\n",
    ")\n",
    "\n",
    "# Set pipeline model explainability check step\n",
    "model_explainability_step = ClarifyCheckStep(\n",
    "    name=\"ModelExplainabilityCheck\",\n",
    "    clarify_check_config=model_explainability_config,\n",
    "    check_job_config=check_job_config,\n",
    "    skip_check=skip_check_model_explainability_param,\n",
    "    register_new_baseline=register_new_baseline_model_explainability_param,\n",
    "    supplied_baseline_constraints=supplied_baseline_constraints_model_explainability_param\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Step\n",
    "\n",
    "In production systems, not all trained models are deployed. Usually, only models that perform better than the threshold for a chosen evaluation metric are deployed. In this step, you will build a Python script that scores the model on a test set using the **Receiver Operating Characteristic Area Under the Curve (ROC-AUC)** metric. The performance of the model against this metric is used in a subsequent step to determine if the model should be registered and deployed. Copy, paste, and run the following code to build an evaluation script that ingests a test dataset and generates the **AUC** metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    # The name of the file should match how the model was saved in the training script\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    test_local_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df_test = pd.read_csv(test_local_path)\n",
    "    \n",
    "    # Extract test set target column\n",
    "    y_test = df_test.iloc[:, 0].values\n",
    "   \n",
    "    cols_when_train = model.feature_names\n",
    "    # Extract test set feature columns\n",
    "    X = df_test[cols_when_train].copy()\n",
    "    X_test = xgb.DMatrix(X)\n",
    "\n",
    "    logger.info(\"Generating predictions for test data.\")\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate model evaluation score\n",
    "    logger.debug(\"Calculating ROC-AUC score.\")\n",
    "    auc = roc_auc_score(y_test, pred)\n",
    "    metric_dict = {\n",
    "        \"classification_metrics\": {\"roc_auc\": {\"value\": auc}}\n",
    "    }\n",
    "    \n",
    "    # Save model evaluation metrics\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Writing evaluation report with ROC-AUC: %f\", auc)\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(metric_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following code block to instantiate the processor and the SageMaker Pipelines step to run the evaluation script. To process the custom script, you use a **ScriptProcessor**. The SageMaker Pipelines **ProcessingStep** function takes the following arguments: the processor, the S3 input location for the test dataset, the model artifact, and the output location to store evaluation results. Additionally, a **property_files** argument is provided. You use property files to store information from the output of the processing step, which in this case is a json file with the model performance metric. As shown later in the tutorial, this is particularly useful to determine when a conditional step should run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model evaluation script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"evaluate.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/evaluate.py\"\n",
    ")\n",
    "\n",
    "eval_processor = ScriptProcessor(\n",
    "    image_uri=training_image,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=predictor_instance_type,\n",
    "    instance_count=predictor_instance_count,\n",
    "    base_job_name=f\"{base_job_name_prefix}-model-eval\",\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"FraudDetEvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "# Set model evaluation step\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"XGBModelEvaluate\",\n",
    "    processor=eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where train step saved model artifacts\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where processing step saved test data\n",
    "            source=process_step.properties.ProcessingOutputConfig.Outputs[\"test_data\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(destination=f\"{model_eval_output_uri}\", output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model and Drift Checking\n",
    "\n",
    "With [SageMaker Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html), you can catalog models, manage model versions, and selectively deploy models to production. Copy, paste, and run the following code block to set up the model registry step. The two parameters, **model_metrics** and **drift_check_baselines**, contain baseline metrics calculated previously in the tutorial by the **ClarifyCheckStep** function. You can also provide your own custom baseline metrics. The intention behind these parameters is to provide a way to configure the baselines associated with a model so they can be used in drift checks and model-monitoring jobs. Each time a pipeline is executed, you can choose to update these parameters with newly calculated baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch baseline constraints to record in model registry\n",
    "model_metrics = ModelMetrics(\n",
    "    bias_post_training=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\"\n",
    "    ),\n",
    "    explainability=MetricsSource(\n",
    "        s3_uri=model_explainability_step.properties.CalculatedBaselineConstraints,\n",
    "        content_type=\"application/json\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fetch baselines to record in model registry for drift check\n",
    "drift_check_baselines = DriftCheckBaselines(\n",
    "    bias_post_training_constraints=MetricsSource(\n",
    "        s3_uri=model_bias_check_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_constraints=MetricsSource(\n",
    "        s3_uri=model_explainability_step.properties.BaselineUsedForDriftCheckConstraints,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    "    explainability_config_file=FileSource(\n",
    "        s3_uri=model_explainability_config.monitoring_analysis_config_uri,\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define register model step\n",
    "register_step = RegisterModel(\n",
    "    name=\"XGBRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    # Fetching S3 location where train step saved model artifacts\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[predictor_instance_type],\n",
    "    transform_instances=[predictor_instance_type],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status_param,\n",
    "    # Registering baselines metrics that can be used for model monitoring\n",
    "    model_metrics=model_metrics,\n",
    "    drift_check_baselines=drift_check_baselines\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Step\n",
    "\n",
    "With Amazon SageMaker, a registered model can be deployed for inference in several ways. In this step, you deploy the model using the **LambdaStep** function. Although you should generally use [SageMaker Projects](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects-whatis.html) for robust model deployments that follow CI/CD best practices, there may be circumstances where it makes sense to use **LambdaStep** for lightweight model deployments to development, test, and internal endpoints serving low traffic volumes. The **LambdaStep** function provides a native integration with AWS Lambda, so you can implement custom logic in your pipeline without provisioning or managing servers. In the context of SageMaker Pipelines, **LambdaStep** enables you to add an AWS Lambda function to your pipelines to support arbitrary compute operations, especially lightweight operations that have short duration. Keep in mind that in a SageMaker Pipelines **LambdaStep**, a Lambda function is limited to 10 minutes maximum runtime, with a modifiable default timeout of 2 minutes. \n",
    "\n",
    "You have two ways to add a **LambdaStep** to your pipelines. First, you can supply the ARN of an existing Lambda function that you created with the AWS Cloud Development Kit (AWS CDK), AWS Management Console, or otherwise. Second, the high-level SageMaker Python SDK has a Lambda helper convenience class that you can use to create a new Lambda function along with your other code defining your pipeline. You use the second method in this tutorial. Copy, paste, and run the following code to define the Lambda handler function. This is the custom Python script that takes in model attributes, such as model name, and deploys to a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_deployer.py\n",
    "\n",
    "\"\"\"\n",
    "Lambda function creates an endpoint configuration and deploys a model to real-time endpoint. \n",
    "Required parameters for deployment are retrieved from the event object\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    # Details of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "    instance_type = event[\"instance_type\"]\n",
    "    instance_count = event[\"instance_count\"]\n",
    "    primary_container = {\"ModelPackageName\": model_package_arn}\n",
    "\n",
    "    # Create model\n",
    "    model = sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer=primary_container,\n",
    "        ExecutionRoleArn=role\n",
    "    )\n",
    "\n",
    "    # Create endpoint configuration\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"Alltraffic\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": instance_count,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create endpoint\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        EndpointConfigName=endpoint_config_name\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code block to create the **LambdaStep**. All parameters such as model, endpoint name, and deployment instance type and count are provided using the **inputs** argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function name must contain sagemaker\n",
    "function_name = \"sagemaker-fraud-det-demo-lambda-step\"\n",
    "# Define Lambda helper class can be used to create the Lambda function required in the Lambda step\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=sagemaker_role,\n",
    "    script=\"lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The inputs used in the lambda handler are passed through the inputs argument in the \n",
    "# LambdaStep and retrieved via the `event` object within the `lambda_handler` function\n",
    "\n",
    "lambda_deploy_step = LambdaStep(\n",
    "    name=\"LambdaStepRealTimeDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": pipeline_model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"model_package_arn\": register_step.steps[0].properties.ModelPackageArn,\n",
    "        \"role\": sagemaker_role,\n",
    "        \"instance_type\": deploy_instance_type_param,\n",
    "        \"instance_count\": deploy_instance_count_param\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the result\n",
    "\n",
    "In this step, you use the **ConditionStep** to compare the current model’s performance based on the Area Under Curve (AUC) metric. Only if the performance is greater than or equal to a threshold AUC (here chosen to be 0.7), the pipeline will perform bias and explainability checks, register the model, and deploy it. Conditional steps like this one help in selective deployment of the best models to production. Copy, paste, and run the following code to define the conditional step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on test set\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"classification_metrics.roc_auc.value\",\n",
    "    ),\n",
    "    right=0.7, # Threshold to compare model performance against\n",
    ")\n",
    "condition_step = ConditionStep(\n",
    "    name=\"CheckFraudDetXGBEvaluation\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[create_model_step, model_bias_check_step, model_explainability_step, register_step, lambda_deploy_step], \n",
    "    else_steps=[]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Run The Pipeline\n",
    "\n",
    "After defining all of the component steps, you can assemble them into a SageMaker Pipelines object. There is no need for you to specify the order of execution since SageMaker Pipelines automatically infers the execution sequence based on the dependencies between the steps.\n",
    "\n",
    "Run the following code to set up the pipeline. The pipeline definition takes all the parameters you defined previously, and the list of component steps. Steps such as create model, bias and explainability checks, model registration, and lambda deployment are not listed in the pipeline definition because they do not run unless the conditional step evaluates to true. If conditional step is true, the subsequent steps are run in order based on their specified inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Pipeline with all component steps and parameters\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[process_instance_type_param, \n",
    "                train_instance_type_param, \n",
    "                train_instance_count_param, \n",
    "                deploy_instance_type_param,\n",
    "                deploy_instance_count_param,\n",
    "                clarify_instance_type_param,\n",
    "                skip_check_model_bias_param,\n",
    "                register_new_baseline_model_bias_param,\n",
    "                supplied_baseline_constraints_model_bias_param,\n",
    "                skip_check_model_explainability_param,\n",
    "                register_new_baseline_model_explainability_param,\n",
    "                supplied_baseline_constraints_model_explainability_param,\n",
    "                model_approval_status_param],\n",
    "    steps=[\n",
    "        process_step,\n",
    "        train_step,\n",
    "        evaluation_step,\n",
    "        condition_step\n",
    "    ],\n",
    "    sagemaker_session=sess\n",
    "    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code in a cell in your notebook. If the pipeline already exists, the code updates the pipeline. If the pipeline doesn't exist, it creates a new one. Ignore any SageMaker SDK warnings such as \"No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "# Full Pipeline description\n",
    "pipeline_definition = json.loads(pipeline.describe()['PipelineDefinition'])\n",
    "pipeline_definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Pipeline process\n",
    "\n",
    "Now, we can check the pipeline by opening the Home Button\n",
    "\n",
    "Click the Home Button at the top > Pipelines\n",
    "![Pipelines](/Images/SMPipeline/Notebook/2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the pipeline by running the following code statement. Pipeline execution parameters are provided as arguments in this step. Go to the **SageMaker Resources** tab on the left panel, select **Pipelines** from the dropdown list, and then choose **FraudDetectXGBPipeline**, **Executions**. The current run of the pipeline is listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Pipeline\n",
    "start_response = pipeline.start(parameters=dict(\n",
    "    SkipModelBiasCheck=True,\n",
    "    RegisterNewModelBiasBaseline=True,\n",
    "    SkipModelExplainabilityCheck=True,\n",
    "    RegisterNewModelExplainabilityBaseline=True)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model result\n",
    "\n",
    "After the **Endpoint status** changes to **InService**, run the following code to invoke the endpoint and run sample inferences. The code returns the model predictions of the first five samples in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch test data to run predictions with the endpoint\n",
    "test_df = pd.read_csv(f\"{processing_output_uri}/test_data/test.csv\")\n",
    "\n",
    "# Create SageMaker Predictor from the deployed endpoint\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name, \n",
    "                                          sagemaker_session=sess,\n",
    "                                          serializer=CSVSerializer(),\n",
    "                                          deserializer=CSVDeserializer()\n",
    "                                         )\n",
    "# Test endpoint with payload of 5 samples\n",
    "payload = test_df.drop([\"fraud\"], axis=1).iloc[:5]\n",
    "result = predictor.predict(payload.values)\n",
    "prediction_df = pd.DataFrame()\n",
    "prediction_df[\"Prediction\"] = result\n",
    "prediction_df[\"Label\"] = test_df[\"fraud\"].iloc[:5].values\n",
    "prediction_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Resources\n",
    "\n",
    "It is a best practice to delete resources that you are no longer using so that you don't incur unintended charges.\n",
    "\n",
    "Please uncomment the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Lambda function\n",
    "#func.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "#sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "# Delete the EndpointConfig\n",
    "#sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "\n",
    "# Delete the model\n",
    "#sm_client.delete_model(ModelName=pipeline_model_name)\n",
    "\n",
    "# Delete the pipeline\n",
    "#sm_client.delete_pipeline(PipelineName=pipeline_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
